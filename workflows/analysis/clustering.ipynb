{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Ouray County Parcel Risk\n",
    "**Author:** Bryce A Young  \n",
    "**Created:** 2025-01-17 | \n",
    "**Modified:** 2025-01-17  \n",
    "\n",
    "#### Overview\n",
    "In this notebook, I do two things: \n",
    "1. I use unsupervized learning models to cluster home types **without** risk score as a variable. I then draw comparisons between clusters and average risk scores. \n",
    "2. I use PCA and t-SNE to reduce the dimensionality of the dataset to derive home archetypes and test the reduced dataset with supervized risk prediction methods.\n",
    "\n",
    "*NOTE: It's possible that both the supervized and unsupervized methods can work together. The supervized method generates a risk score, then the clustering groups homes into archetypes, then we can go back and assign archetypes to homes and assess how many of those homes burned in historic fires such as Palisades, Lahaina, Marshall and Camp.*\n",
    "\n",
    "## Clustering\n",
    "Clustering can be used to reveal structure between samples of data and assign group membership to similar groups of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Create / load the datasets:\n",
    "n_samples = 1500\n",
    "X0, _ = make_blobs(n_samples=n_samples, centers=2, n_features=2, random_state=0)\n",
    "X1, _ = make_blobs(n_samples=n_samples, centers=5, n_features=2, random_state=0)\n",
    "\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1.3)\n",
    "transformation = [[0.6, -0.6], [-0.2, 0.8]]\n",
    "X2 = np.dot(X, transformation)\n",
    "X3, _ = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\n",
    "X4, _ = make_moons(n_samples=n_samples, noise=.12)\n",
    "\n",
    "X = [X0, X1, X2, X3, X4]\n",
    "# The datasets are X[i], where i ranges from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Code to plot clusters\n",
    "################################\n",
    "def plot_cluster(ax, data, cluster_assignments):\n",
    "    '''Plot two-dimensional data clusters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib axis\n",
    "        Axis to plot on\n",
    "    data : list or numpy array of size [N x 2] \n",
    "        Clustered data\n",
    "    cluster_assignments : list or numpy array [N]\n",
    "        Cluster assignments for each point in data\n",
    "\n",
    "    '''\n",
    "    clusters = np.unique(cluster_assignments)\n",
    "    n_clusters = len(clusters)\n",
    "    for ca in clusters:\n",
    "        kwargs = {}\n",
    "        if ca == -1:\n",
    "            # if samples are not assigned to a cluster (have a cluster assignment of -1, color them gray)\n",
    "            kwargs = {'color':'gray'}\n",
    "            n_clusters = n_clusters - 1\n",
    "        ax.scatter(data[cluster_assignments==ca, 0], data[cluster_assignments==ca, 1],s=5,alpha=0.5, **kwargs)\n",
    "        ax.set_xlabel('feature 1')\n",
    "        ax.set_ylabel('feature 2')\n",
    "        ax.set_title(f'No. Clusters = {n_clusters}')\n",
    "        ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# Plot each dataset in a separate subplot\n",
    "for i, (dataset, title) in enumerate(zip(X, [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\"])):\n",
    "    axes[i].scatter(dataset[:, 0], dataset[:, 1], s=10)\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('equal')  # Set equal scaling for better visualization\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have viewed the basic shape of the data, we can create elbow curves for each dataset to determine what $k$ value to set for k-means clustering. (Refer to machine learning assignment 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "fig.suptitle('Elbow Curves for Different Datasets')\n",
    "\n",
    "# Create an inertia list for each dataset\n",
    "for i, dataset in enumerate(X):\n",
    "    sumsqs = []\n",
    "    \n",
    "    # Iterate through cluster sizes and calculate inertia, appending to inertia list   \n",
    "    for k in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(dataset)\n",
    "        sumsqs.append(kmeans.inertia_)\n",
    "        \n",
    "    # Plot the elbow curve on the corresponding subplot\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axes[row, col].plot(range(1, 11), sumsqs, marker='o')\n",
    "    axes[row, col].set_title(f'Elbow Curve for X{i}')\n",
    "    axes[row, col].set_xlabel('Number of Clusters (k)')\n",
    "    axes[row, col].set_ylabel('Sum of Squares')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### K MEANS CLUSTERING ####\n",
    "\n",
    "# List of best k values for each dataset\n",
    "best_kmeans = [2, 4, 3, 3, 2]\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "fig.suptitle('KMeans Clustered Data')\n",
    "\n",
    "# Iterate over datasets and corresponding best k values\n",
    "for i, (dataset, best_k) in enumerate(zip(X, best_kmeans)):\n",
    "    # Fit the specified k-means model\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=0)\n",
    "    cluster_assignments = kmeans.fit_predict(dataset)\n",
    "    \n",
    "    # Plot the clustered data on the corresponding subplot\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axes[row, col].scatter(dataset[:, 0], dataset[:, 1], c=cluster_assignments, cmap='viridis', s=10)\n",
    "    axes[row, col].set_title(f'X{i} (k={best_k})')\n",
    "    axes[row, col].set_xlabel('Feature 1')\n",
    "    axes[row, col].set_ylabel('Feature 2')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DBSCAN CLUSTERING ####\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "fig.suptitle('DBSCAN Clustered Data')\n",
    "\n",
    "# Iterate over datasets\n",
    "for i, dataset in enumerate(X):\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=0.3, min_samples=4)\n",
    "    cluster_assignments = dbscan.fit_predict(dataset)\n",
    "    \n",
    "    # Plot the clustered data on the corresponding subplot\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axes[row, col].scatter(dataset[:, 0], dataset[:, 1], c=cluster_assignments, cmap='viridis', s=10)\n",
    "    axes[row, col].set_title(f'X{i}')\n",
    "    axes[row, col].set_xlabel('Feature 1')\n",
    "    axes[row, col].set_ylabel('Feature 2')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SPECTRAL CLUSTERING ####\n",
    "\n",
    "from sklearn.cluster import SpectralClustering as spc\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "fig.suptitle('Spectral Clustered Data')\n",
    "\n",
    "# Iterate over datasets and corresponding best k values\n",
    "for i, (dataset, n) in enumerate(zip(X, best_kmeans)):\n",
    "    # Fit the specified spectral clustering model\n",
    "    spectral = spc(n_clusters=n)\n",
    "    cluster_assignments = spectral.fit_predict(dataset)\n",
    "    \n",
    "    # Plot the clustered data on the corresponding subplot\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axes[row, col].scatter(dataset[:, 0], dataset[:, 1], c=cluster_assignments, cmap='viridis', s=10)\n",
    "    axes[row, col].set_title(f'X{i} (k={n})')\n",
    "    axes[row, col].set_xlabel('Feature 1')\n",
    "    axes[row, col].set_ylabel('Feature 2')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Here we use Principal Components Analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) to reduce the dimensionality of the dataset. Then we will compare the two techniques and assess which tended to cluster best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# load dataset\n",
    "digits = datasets.load_digits()\n",
    "n_sample = digits.target.shape[0]\n",
    "n_feature = digits.images.shape[1] * digits.images.shape[2]\n",
    "X_digits = np.zeros((n_sample, n_feature))\n",
    "for i in range(n_sample):\n",
    "    X_digits[i, :] = digits.images[i, :, :].flatten()\n",
    "y_digits = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dimensions of the dataset\n",
    "num_samples, num_features = X_digits.shape\n",
    "\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 2  # Number of components for PCA\n",
    "pca = PCA(n_components = n_components)\n",
    "X_pca = pca.fit_transform(X_digits)\n",
    "\n",
    "# Plot the data in 2D space with labels\n",
    "plt.figure(figsize=(7.5, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    indices = (y_digits == i)\n",
    "    plt.scatter(X_pca[indices, 0], X_pca[indices, 1], label=str(i), s=20)\n",
    "\n",
    "plt.title('PCA of Digits Dataset (2D)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Digit', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation: cumulative fraction of variance explained by principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca1 = PCA()\n",
    "pca1.fit(X_digits)\n",
    "\n",
    "# Calculate cumulative fraction of variance explained\n",
    "cumulative_var_ratio = np.cumsum(pca1.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative fraction of variance explained\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, len(cumulative_var_ratio) + 1), cumulative_var_ratio, marker='.', linestyle='-')\n",
    "plt.title('Cumulative Fraction of Variance Explained by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Fraction of Variance Explained')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fraction of variance explained by the first 2 principal components\n",
    "fraction_variance_explained = pca.explained_variance_ratio_\n",
    "\n",
    "# Print the result\n",
    "print(\"Fraction of Variance Explained by the First 2 Principal Components:\")\n",
    "print(f\"Principal Component 1: {fraction_variance_explained[0]:.4f}\")\n",
    "print(f\"Principal Component 2: {fraction_variance_explained[1]:.4f}\")\n",
    "print(f\"Total Cumulative Variance Unexplained: {(1-(fraction_variance_explained[0] + fraction_variance_explained[1])): .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, \n",
    "            perplexity = 25, \n",
    "            n_iter = 500,\n",
    "            random_state=0\n",
    "           )\n",
    "X_tsne = tsne.fit_transform(X_digits)\n",
    "\n",
    "# Plot the data in 2D with associated labels\n",
    "plt.figure(figsize=(7.5, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    indices = (y_digits == i)\n",
    "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=str(i), s=15)\n",
    "\n",
    "plt.title('t-SNE of Digits Dataset (2D) with True Labels')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Digit', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parcel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
