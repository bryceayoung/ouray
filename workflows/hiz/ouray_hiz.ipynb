{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec583c1c-12ee-4fbb-951e-f0475ae51eac",
   "metadata": {},
   "source": [
    "# Ouray Defensible Space Analysis\n",
    "\n",
    "Author: **Bryce A Young** | \n",
    "Created: **2024-12-05** | \n",
    "Modified: **2024-12-17**\n",
    "\n",
    "In this notebook, we analyze fuel distributions within the defensible space of every building in Ouray County.\n",
    "\n",
    "This analysis workflow also documents cleaning and preparing raw data for the analysis.\n",
    "\n",
    "#### Data \n",
    "- (vector) Microsoft Building Footprints\n",
    "- (raster) SILVIS Global WUI raster\n",
    "- (raster) LiDAR-derived rasters\n",
    "\n",
    "#### Workflow \n",
    "- Create HIZ boundaries around each building footprint\n",
    "- Count number of homes within each HIZ\n",
    "- Obtain WUI class for each home\n",
    "- Get zonal summary values of each LiDAR-derived raster for each HIZ\n",
    "\n",
    "## Step 0: Setup Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f366a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "### Directory ###\n",
    "# Repository root\n",
    "root = 'E:/_PROJECTS/Ouray_ParcelRisk/ouray'\n",
    "\n",
    "### Data paths ###\n",
    "# Folder where all the data inputs and outputs will live\n",
    "data = 'E:/_PROJECTS/Ouray_ParcelRisk/data'\n",
    "# Folder for all SILVIS WUI files\n",
    "silvis_wui = os.path.join(data, 'silvis_wui')\n",
    "# Folder for microsoft building footprints\n",
    "mbf = os.path.join(data, 'building_footprints')\n",
    "# Folder containing LiDAR-derived rasters\n",
    "tiffs_from_las = os.path.join(data, 'tiffs_from_las')\n",
    "# Folder countaining the Ouray County boundary\n",
    "county_boundary = os.path.join(data, 'county_boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05a1593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\_PROJECTS\\\\Ouray_ParcelRisk\\\\ouray'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ef97a",
   "metadata": {},
   "source": [
    "## Step 1: Create HIZ Boundaries around each Structure\n",
    "---\n",
    "**Background: obtaining building footprints**  \n",
    "*I searched \"Ouray County Colorado Microsoft Building Footprints\" and found a Colorado state website that had parsed the Microsoft Building Footprints dataset into county datasets for all of Colorado. So I was able to directly download the Ouray footprints without having to do any data manipulation myself.*\n",
    "\n",
    "*After downloading the footprints, I added them to ArcGIS Pro and viewed them on top of my LiDAR data. Especially the `zentropy` layer shows buildings very well. I noticed that the footprint geometries are offset from the location of the buildings in the LiDAR rasters. So I used 'rubber sheeting' in ArcGIS Pro to put control points and target points that were intended to bring the building footprints closer to the buildings in the LiDAR rasters.*\n",
    "\n",
    "*The resulting shapefile was saved to `ouray_footprints_rs.shp` where 'rs' denotes 'rubber sheet' transformation of the original footprint layer. Original footprints are saved in the same directory as `Ouray_County_Buildings.shp`.*\n",
    "\n",
    "### Import geometries and run HIZ script\n",
    "\n",
    "**TO DO**: I have to re-do this rubber sheeting and make sure to save the result as its own shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53841908-5925-4775-99ea-c55c5a8df894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Import building footprints\n",
    "footprints = gpd.read_file(os.path.join(mbf, 'Ouray_County_Buildings.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c01c3-9a1f-45ed-b610-7feba37d730e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\_PROJECTS\\\\Ouray_ParcelRisk\\\\ouray\\\\workflows'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.HIZ import get_hiz\n",
    "\n",
    "# Create defensible space zones around all building footprints\n",
    "hiz = get_hiz(footprints)\n",
    "hiz.head() # Preview data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd62c1f",
   "metadata": {},
   "source": [
    "Now that we have created the HIZ boundaries around each building footprints, we need to save each one as its own geopackage for analysis. This could also be a shapefile, but a geopackage is more modern and generally preferred in 2025, so we'll stick with that.\n",
    "\n",
    "Because of the difference in alignment between buildings in the LiDAR data and the outlines provided by Microsoft Building Footprints, we're actually going to use this first buffer as an analog for the building itself. This will help avoid incuding the building pixels from the raster in the analysis.\n",
    "\n",
    "Below, we save each geometry column as its own geopackage, then we will restart the kernel to save memory and re-import the individual files.\n",
    "\n",
    "**TO DO**: If I am using buffer_z1 as the actual footprint, then I'm going to want to rename these to keep everything straight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each as its own geopackage\n",
    "\n",
    "# In order to save a shapefile of HIZs, each geometry column needs to be saved as a separate geopackage\n",
    "# Make sure each file contains one geometry column, and also 'FID' and \"County'.\n",
    "geom_cols = ['geometry', 'buffer_z1', 'buffer_z2', 'buffer_z3']\n",
    "non_geom_cols = ['FID', 'County']\n",
    "\n",
    "# Iterate and save each as a separate Shapefile\n",
    "for geom_col in geom_cols:\n",
    "    # Create a temporary GeoDataFrame with only the current geometry column and non-geometry columns\n",
    "    temp_gdf = hiz[non_geom_cols + [geom_col]].copy()\n",
    "    temp_gdf.set_geometry(geom_col, inplace=True)  # Loop through columns, setting each as the geometry column\n",
    "    \n",
    "    # Save to Shapefile\n",
    "    temp_gdf.to_file(f'E:\\_PROJECTS\\Ouray_ParcelRisk\\data\\hiz_geoms\\hiz_{geom_col}.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HIZ geoms into environment\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "z1 = gpd.read_file(\"hiz_buffer_z1.gpkg\")\n",
    "z2 = gpd.read_file(\"hiz_buffer_z2.gpkg\")\n",
    "z3 = gpd.read_file(\"hiz_buffer_z3.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1c559",
   "metadata": {},
   "source": [
    "## Step 2: Count the Number of Adjacent Structures\n",
    "---\n",
    "\n",
    "In order to obtain a proxy for structure density, we are going to count the number of adjacent structures in the vicinity of each home. We will append these counts to the geopackages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff055bd-76ea-44c8-896b-98538cefe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.HIZ import structures_per_zone\n",
    "\n",
    "buffer_cols = ['buffer_z1', 'buffer_z2', 'buffer_z3']  # Define your buffer zone columns\n",
    "counts_df = structures_per_zone(gdf, 'footprints', buffer_cols)\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc882c",
   "metadata": {},
   "source": [
    "**TO DO**: Does `counts_df` have geometries? Might need to add the extra step of appending that info to the footprint gdf. I think I ran this function in Gunnison so I should go look at those notebooks. \n",
    "\n",
    "## Step 3: Attributing WUI type to each home\n",
    "---\n",
    "\n",
    "#### Creating a WUI type raster for Ouray County\n",
    "Right now I have the raw tiles from SILVIS GLobal WUI. Two tiles are required to cover the entire county area. I downloaded the entire North America dataset and found the correct tiles to cover the area. These are the raw data used here. \n",
    "\n",
    "First let's import the tiles and preview their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3363980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr1:  {'driver': 'GTiff', 'dtype': 'uint8', 'nodata': 255.0, 'width': 10000, 'height': 10000, 'count': 1, 'crs': CRS.from_wkt('PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"latitude_of_center\",52],PARAMETER[\"longitude_of_center\",-97.5],PARAMETER[\"false_easting\",8264722.17686],PARAMETER[\"false_northing\",4867518.35323],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(10.0, 0.0, 7300000.0,\n",
      "       0.0, -10.0, 3500000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "arr2:  {'driver': 'GTiff', 'dtype': 'uint8', 'nodata': 255.0, 'width': 10000, 'height': 10000, 'count': 1, 'crs': CRS.from_wkt('PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"latitude_of_center\",52],PARAMETER[\"longitude_of_center\",-97.5],PARAMETER[\"false_easting\",8264722.17686],PARAMETER[\"false_northing\",4867518.35323],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(10.0, 0.0, 7300000.0,\n",
      "       0.0, -10.0, 3400000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "# Import Global WUI tiles\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from utils.raster import read_raster\n",
    "\n",
    "# Read in arrays and extract metadata\n",
    "wui1 = os.path.join(silvis_wui, 'X0073_Y0064', 'wui.tif')\n",
    "wui2 = os.path.join(silvis_wui, 'X0073_Y0065', 'wui.tif')\n",
    "\n",
    "arr1, prof1 = read_raster(wui1, layer=1, profile=True)\n",
    "arr2, prof2 = read_raster(wui2, layer=1, profile=True)\n",
    "\n",
    "# Preview metadata\n",
    "print('arr1: ', prof1)\n",
    "print('arr2: ', prof2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dc3a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac547b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: PROJCS[\"NAD83_Conus_Albers\",GEOGCS[\"GCS_NAD83\",DAT ...>\n",
       "Name: NAD83_Conus_Albers\n",
       "Axis Info [cartesian]:\n",
       "- [east]: Easting (metre)\n",
       "- [north]: Northing (metre)\n",
       "Area of Use:\n",
       "- undefined\n",
       "Coordinate Operation:\n",
       "- name: unnamed\n",
       "- method: Albers Equal Area\n",
       "Datum: North American Datum 1983\n",
       "- Ellipsoid: GRS 1980\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c036fdd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a08be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22f3a641",
   "metadata": {},
   "source": [
    "### Clip WUI rasters to Analysis Area (county boundary) - put this section in raster_prep.ipynb\n",
    "\n",
    "Now we're going to clip the WUI rasters to the county boundary, setting values to nodata where county_boundary_raster == 0. The county boundary will be the snap raster and the source of truth for raster extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject WUI rasters to same CRS as county boundary\n",
    "# NOTE: make sure it's also the same CRS as the homes\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.warp import transform_geom\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# Paths to your files\n",
    "raster_path = 'path/to/your/raster.tif'\n",
    "vector_path = 'path/to/your/vector_boundary.shp'\n",
    "output_path = 'path/to/output/clipped_raster.tif'\n",
    "\n",
    "# Set desired nodata value\n",
    "nodata_value = 255\n",
    "\n",
    "# Step 1: Read vector boundary and reproject to raster CRS\n",
    "vector = gpd.read_file(vector_path)\n",
    "\n",
    "# Open the raster to get its CRS\n",
    "with rasterio.open(raster_path) as src:\n",
    "    raster_crs = src.crs\n",
    "\n",
    "# Reproject vector boundary to match raster CRS\n",
    "vector_reprojected = vector.to_crs(raster_crs)\n",
    "\n",
    "# Step 2: Clip the raster using the reprojected vector geometry\n",
    "geometries = [geom.__geo_interface__ for geom in vector_reprojected.geometry]\n",
    "\n",
    "# Open raster and clip\n",
    "with rasterio.open(raster_path) as src:\n",
    "    # Perform the clipping\n",
    "    clipped_raster, clipped_transform = mask(src, geometries, crop=True, nodata=nodata_value)\n",
    "    \n",
    "    # Update the metadata\n",
    "    clipped_meta = src.meta.copy()\n",
    "    clipped_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": clipped_raster.shape[1],\n",
    "        \"width\": clipped_raster.shape[2],\n",
    "        \"transform\": clipped_transform,\n",
    "        \"nodata\": nodata_value\n",
    "    })\n",
    "\n",
    "# Step 3: Replace values outside the mask (optional but ensures consistency)\n",
    "clipped_raster = np.where(clipped_raster == nodata_value, nodata_value, clipped_raster)\n",
    "\n",
    "# Step 4: Save the clipped raster\n",
    "with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "    dst.write(clipped_raster)\n",
    "\n",
    "print(f\"Clipped raster saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75cb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.raster import clip_raster\n",
    "\n",
    "wui = clip_raster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new WUI array to file\n",
    "from utils.raster import write_raster\n",
    "\n",
    "write_raster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d640ad0",
   "metadata": {},
   "source": [
    "### Get WUI raster values per home\n",
    "\n",
    "Now that we have the WUI rasters clipped and in the right CRS, we're going to find the value of the raster at the centriod of each home. Those values will be appended to the HIZ geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b283c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to do get the centriod of each geometry and get the raster value at the centriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append values to gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gdf to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff66adf",
   "metadata": {},
   "source": [
    "## Step 4: Get canopy cover info for each HIZ\n",
    "---\n",
    "\n",
    "### Read in LiDAR-derived rasters\n",
    "\n",
    "I created these rasters in R using the **lidR** package and methods documented in this repository under `r_workflows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60957165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-2m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "2-4m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "4-8m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "8+m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "chm:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "ladder:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "density:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N + NAVD88 height - US Geoid Model of 2018\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "zentropy:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from utils.raster import read_raster\n",
    "\n",
    "# Setup environment\n",
    "# ----\n",
    "# Path to all las-derived rasters\n",
    "tiffs_from_las = r\"E:/_PROJECTS/Ouray_ParcelRisk/data/tiffs_from_las\"\n",
    "# County, municipality etc. where the rasters cover\n",
    "aoi = 'ouray'\n",
    "# ----\n",
    "\n",
    "# Initiate list to store objects\n",
    "lidar_rasters = []\n",
    "lidar_raster_profiles = []\n",
    "\n",
    "# Canopy cover 0-2m\n",
    "cc0_2, cc0_2_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc0_2m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc0_2)\n",
    "lidar_raster_profiles.append(cc0_2_profile)\n",
    "\n",
    "# Canopy cover 2-4m\n",
    "cc2_4, cc2_4_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc2_4m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc2_4)\n",
    "lidar_raster_profiles.append(cc2_4_profile)\n",
    "\n",
    "# Canopy cover 4-8m\n",
    "cc4_8, cc4_8_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc4_8m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc4_8)\n",
    "lidar_raster_profiles.append(cc4_8_profile)\n",
    "\n",
    "# Canopy cover 8-40m\n",
    "cc8_40, cc8_40_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc8_40m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc8_40)\n",
    "lidar_raster_profiles.append(cc8_40_profile)\n",
    "\n",
    "# Canopy height model\n",
    "chm, chm_profile = read_raster(os.path.join(tiffs_from_las, '{0}_chm.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(chm)\n",
    "lidar_raster_profiles.append(chm_profile)\n",
    "\n",
    "# Ladder fuels\n",
    "ladder, ladder_profile = read_raster(os.path.join(tiffs_from_las, '{0}_ladderfuels.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(ladder)\n",
    "lidar_raster_profiles.append(ladder_profile)\n",
    "\n",
    "# Point density\n",
    "density, density_profile = read_raster(os.path.join(tiffs_from_las, '{0}_pointDensity.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(density)\n",
    "lidar_raster_profiles.append(density_profile)\n",
    "\n",
    "# Z-Entropy\n",
    "zentropy, zentropy_profile = read_raster(os.path.join(tiffs_from_las, '{0}_zentropy.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(zentropy)\n",
    "lidar_raster_profiles.append(zentropy_profile)\n",
    "\n",
    "# Print metadata for quality inspection\n",
    "print('0-2m: ', cc0_2_profile)\n",
    "print('2-4m: ', cc2_4_profile)\n",
    "print('4-8m: ', cc4_8_profile)\n",
    "print('8+m: ', cc8_40_profile)\n",
    "print('chm: ', chm_profile)\n",
    "print('ladder: ', ladder_profile)\n",
    "print('density: ', density_profile)\n",
    "print('zentropy: ', zentropy_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f99d791",
   "metadata": {},
   "source": [
    "### put this section in raster_prep.ipynb!\n",
    "\n",
    "The metadata of these rasters needs updating. For instance, we can change the dtype on many of these rasters to save space and processing speed. To do that, we will have to solve for the nodata value as nan, since that requires a float dtype. Ladder fuels are a binary raster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23708c",
   "metadata": {},
   "source": [
    "We want the CRS and shape of these rasters to match the CRS and shape of the buildings and the county raster. So let's do some raster cleanup and make these changes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a0c66",
   "metadata": {},
   "source": [
    "Now let's plot the rasters to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32dd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 4 rasters in 2x2 subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ededcca-8f00-45e0-8717-6eb0f409fe06",
   "metadata": {},
   "source": [
    "Notice the missing pixel values seen in the plot. I fill these in with a 3x3 moving window that fills in missing values with the average value of the 8 surrounding pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02302565-be17-4f1a-a539-dc7cfd23026b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Creating a window required a few steps. First, defining what the window does. I call this function `fill_nan`. I use `generic_filter` from the `scipy` library, which does this automatically, providing three arguments: the **array** to filter, the **function** of the window, and the **size** of the window. Optionally, we can set **mode** based on how we want to treat pixels outside of the array. Let's stick with the default *reflect*. You can read more about options here: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.generic_filter.html]\n",
    "\n",
    "### Fill in missing values - put this section in raster_prep.ipynb\n",
    "\n",
    "#### Canopy Cover 0-2m\n",
    "\n",
    "**TO DO:** This code needs cleaned up so that it uses the class I created, or maybe I just need to simplify things by breaking down the class into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with moving window\n",
    "from scipy.ndimage import generic_filter\n",
    "from utils.raster import FillNaN # NOTE: am I importing this class correctly?\n",
    "\n",
    "# Apply the moving window function to fill missing values in cc0_2\n",
    "window_size = (3, 3)\n",
    "arr_max = generic_filter(arr_hmax, fill_nan, size=window_size, mode='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview to ensure quality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cc0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save array, overwriting\n",
    "from utils.raster import write_raster\n",
    "\n",
    "write_raster('file/path/raster.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d688c28",
   "metadata": {},
   "source": [
    "#### Canopy Cover 2-4m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f931429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with moving window\n",
    "from scipy.ndimage import generic_filter\n",
    "from utils.raster import FillNaN # NOTE: am I importing this class correctly?\n",
    "\n",
    "# Apply the moving window function to fill missing values in cc2_4\n",
    "window_size = (3, 3)\n",
    "arr_max = generic_filter(arr_hmax, fill_nan, size=window_size, mode='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview to ensure quality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cc2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save array, overwriting\n",
    "from utils.raster import write_raster\n",
    "\n",
    "write_raster('file/path/raster.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3054a",
   "metadata": {},
   "source": [
    "#### Canopy Cover 4-8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ff0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with moving window\n",
    "from scipy.ndimage import generic_filter\n",
    "from utils.raster import FillNaN # NOTE: am I importing this class correctly?\n",
    "\n",
    "# Apply the moving window function to fill missing values in cc4_8\n",
    "window_size = (3, 3)\n",
    "arr_max = generic_filter(arr_hmax, fill_nan, size=window_size, mode='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview to ensure quality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cc4_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save array, overwriting\n",
    "from utils.raster import write_raster\n",
    "\n",
    "write_raster('file/path/raster.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83142a8e",
   "metadata": {},
   "source": [
    "#### Canopy Cover 8m+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39264af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with moving window\n",
    "from scipy.ndimage import generic_filter\n",
    "from utils.raster import FillNaN # NOTE: am I importing this class correctly?\n",
    "\n",
    "# Apply the moving window function to fill missing values in cc_8\n",
    "window_size = (3, 3)\n",
    "arr_max = generic_filter(arr_hmax, fill_nan, size=window_size, mode='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview to ensure quality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cc_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save array, overwriting\n",
    "from utils.raster import write_raster\n",
    "\n",
    "write_raster('file/path/raster.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64f03f-78b8-4e7a-b5d3-7ed96ebc5dcd",
   "metadata": {},
   "source": [
    "#### Get raster values per HIZ across the county\n",
    "\n",
    "Recall the geodataframes of home ignition zones that we created at the beginning of this workflow. It contains geometries for **XX** building footprins and their home ignition zones. We're going to look at the vegetation within each HIZ to get an idea of the vegetation profile around each home.\n",
    "\n",
    "For the purposes of reducing wildfire risk to individual homes, it's important that we analyze raster values within each buffer zone. GeoPandas comes in handy for this task.\n",
    "\n",
    "Our workflow for this calculation - written in plain English - is as follows:\n",
    "1. **Mask raster with geometries**: Recall that each home ignition zone gdf is a geoseries of Shapely geometries. We need to iterate through **each** of these **XX** objects and use them to mask our raster, object by object.\n",
    "2. **Calculate average raster values within each HIZ**: once the mask is applied to only include a single building buffer, we'll take the average of all the pixels in the area. \n",
    "3. **Store respective canopy cover value per HIZ as a new column in each HIZ dataframe:** The average values will be stored as columns and each value will correspond to the geometry in the same row.\n",
    "\n",
    "**TO DO:** decide the best way to organize these. I'm thinking column titles will be 'ccvalue_hiz' and will be appended to the individual HIZ shapefiles rather than the grandfather gdf.\n",
    "\n",
    "THIS IS IN `ndvi_hiz.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average raster value per HIZ\n",
    "def calculate_avg(arr):\n",
    "    \n",
    "    # Count non-NaN values in the array\n",
    "    non_nan_count = np.sum(~np.isnan(arr))\n",
    "    # Compute the sum of non-NaN values\n",
    "    non_nan_sum = np.sum(arr[~np.isnan(arr)])\n",
    "    # Calculate the average (handle case when non_nan_count is 0 to avoid division by zero)\n",
    "    avg = non_nan_sum / non_nan_count if non_nan_count > 0 else np.nan\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e7141",
   "metadata": {},
   "source": [
    "#### Append values to HIZ geopackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f847a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to append values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0a13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated GDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912afde3",
   "metadata": {},
   "source": [
    "## Discussion and Conclusions\n",
    "---\n",
    "\n",
    "- Broader picture of wildfire risk to communities and susceptibility and defensible space\n",
    "- Describe the project in its entirety, in brief\n",
    "- Here's how this notebook/workflow fits into the larger project\n",
    "- Here's what we did.\n",
    "- Here were the most difficult parts.\n",
    "- Here were the key parts to get right.\n",
    "- Time estimates for running the code.\n",
    "- Information on computational expense.\n",
    "- Room for improvement\n",
    "\n",
    "## NOTES: Possibble to use this code somewhere?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f6c1-ba55-49e5-835e-2fee694dd209",
   "metadata": {},
   "source": [
    "We're going to clip our rasters to the extent of the given geometry for each of these.\n",
    "\n",
    "Our simplified proxy for NFPA compliance is going to be the **average max height** of vegetation pixels within each zone.\n",
    "\n",
    "The process of clipping rasters to geometries requires a fe steps. First, we're going to mask the max height raster `arr_max` with each HIZ geometry - `z1`, `z2`, and `z3`. To do this, we have to rasterize each geometry. The following 3 code blocks accomplish that, and save the masked raster outputs to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3e50665-abb4-444a-b322-b23848de0270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "# Prepare geometries from GeoDataFrame for masking\n",
    "geoms = z1['geometry'].values # Zone 1\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z1_out_values, z1_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e6e13de-7457-4e23-ad1c-78b415b7c213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = z2['geometry'].values # Zone 2\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z2_out_values, z2_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05f46e07-e12e-40cd-a32d-b16f715aa52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = z3['geometry'].values # Zone 3\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z3_out_values, z3_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1bae4-8974-439a-87f6-da39ab7fc22b",
   "metadata": {},
   "source": [
    "Next, let's define a function for computing the average value of all the pixels.\n",
    "\n",
    "We know that our zones are all donut-shaped. If we divide the sum of the pixels by the size of the array, then pixels both inside and outside the donut will be included in the size, and that will throw off our calculation. The above 3 code blocks have set all the values outside and inside of the donuts to `NaN`, so now we make sure our `calculate_avg` function divided by the number of *non-NaN* pixels in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d55c73-0476-4254-b1d5-75bef0185fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4dcfc00-40ab-4ed4-b77d-c37b8daf5951",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "All that's left to do is calculate the average max height per zone. Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9437e5fe-49f8-4478-8307-e92b6ef6a5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3484971919686135"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z3_out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfb5257f-f10b-4329-beca-3332ddd82a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.054104777520576"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z2_out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bf84c2e-2682-4059-ae5d-f7767a5d7b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9157764165088382"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z1_out_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878ed2c-a044-4a3d-a19b-e14fec8ef2f5",
   "metadata": {},
   "source": [
    "### CONCLUSIONS\n",
    "\n",
    "There are some insights and some caveats to draw from this from a fire mitigation perspective.\n",
    "\n",
    "For a NFPA-compliant home ignition zone, we would expect to see taller average height values further away from the home. Instead, we are seeing the reverse.\n",
    "\n",
    "The Z1 values have possibly been skewed by the home itself. Since the Microsoft building footprint does not perfectly align with the location of the building in the point cloud, it is likely that the building itself has been included in the max height calculation. A way to handle this would be to classify the point cloud to include buildings, and derive the building footprint directly from the LiDAR data.\n",
    "\n",
    "The Z2 and Z3 values are probably more reliable. It tells us that there is tall vegetation surrounding the home. If the average height exceeds 3m in Z2, it is likely that this property could benefit from removing some trees and ensuring that the ground is clear of continuous flammable fuels. Since Z3 average max height is lower, it's likely that Z3 will not carry fire, although the average max height could be skewed by clumps of tall, continuous forest fuels interspersed with grassland, which is typical of this environment.\n",
    "\n",
    "My conclusion is that average max height does not produce an actionable insight for home risk reduction. There is potential to develop better ways of producing actionable insights for home wildfire risk mitigation, but none have been developed (based on a meticulous review of over 150 papers and models). While the results presented here may prove to be a useful predictor in a random forest or multi-layer perceptron model, I will continue to test the value of different LiDAR metrics using the framework developed through this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
