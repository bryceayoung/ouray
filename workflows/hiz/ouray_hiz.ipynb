{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec583c1c-12ee-4fbb-951e-f0475ae51eac",
   "metadata": {},
   "source": [
    "# Ouray Defensible Space Analysis\n",
    "\n",
    "Author: **Bryce A Young** (git bryceayoung) | \n",
    "Created: **2024-12-05** | \n",
    "Modified: **2025-02-11**\n",
    "\n",
    "In this notebook, we analyze fuel distributions within the defensible space of every building in Ouray County.\n",
    "\n",
    "Documents for cleaning and preparing raw data for the analysis are located in this repo under `workflows/data_prep`.\n",
    "\n",
    "#### Data \n",
    "- (vector) Microsoft Building Footprints\n",
    "- (raster) LiDAR-derived rasters\n",
    "\n",
    "#### Workflow \n",
    "- Create HIZ boundaries around each building footprint\n",
    "- Count number of homes within each HIZ\n",
    "- Get zonal summary values of each LiDAR-derived raster for each HIZ\n",
    "\n",
    "## Step 0: Setup Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f366a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\_PROJECTS\\\\P001_OurayParcel\\\\ouray'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "### Directory ###\n",
    "# Repository\n",
    "os.chdir(r'D:/_PROJECTS/P001_OurayParcel/ouray')\n",
    "# Root workspace\n",
    "ws = r'D:/_PROJECTS/P001_OurayParcel'\n",
    "\n",
    "### Data paths ###\n",
    "# Folder where all the data inputs and outputs will live\n",
    "data = os.path.join(ws, 'data')\n",
    "# Folder for geoms (microsoft building footprints, parcels, and county boundary)\n",
    "geoms = os.path.join(data, 'county_geoms')\n",
    "# Folder containing LiDAR-derived rasters\n",
    "rasters = os.path.join(data, 'tiffs_from_las')\n",
    "# Scratch folder\n",
    "scratch = os.path.join(data, '_temp')\n",
    "\n",
    "# Ensure correct working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ef97a",
   "metadata": {},
   "source": [
    "## Step 1: Create HIZ Boundaries around each Structure\n",
    "---\n",
    "**Background: obtaining building footprints**  \n",
    "*I searched \"Ouray County Colorado Microsoft Building Footprints\" and found a Colorado state website that had parsed the Microsoft Building Footprints dataset into county datasets for all of Colorado. So I was able to directly download the Ouray footprints without having to do any data manipulation myself.*\n",
    "\n",
    "*After downloading the footprints, I added them to ArcGIS Pro and viewed them on top of my LiDAR data. Especially the `zentropy` layer shows buildings very well. I noticed that the footprint geometries are offset from the location of the buildings in the LiDAR rasters. So I used the 'Transform' tool in ArcGIS Pro 3.4.0 to put control points and target points (links) with Rubbersheet (natural neighbor) as the transformation method. This operation brought the building footprints closer to the buildings as they appear in the Z-entropy LiDAR raster. This created 105 anchor points througout the county extent where buildings are present.*\n",
    "\n",
    "*The resulting shapefile was saved to `buildings_rs_WKID26913.shp` where 'rs' denotes 'rubber sheet' transformation of the original footprint layer. Original footprints are saved in the same directory as `Ouray_County_Buildings.shp` and geopackage as `buildings_WKID26913.gpkg`.*\n",
    "\n",
    "*In order to display my results, I randomized the building footprints and selected 9 random buildings which I plotted in an ArcGIS Pro layout with a 100m square buffer around the centroid. The maps in the layout show the pre-rubbersheet and post-rubbersheet building footprints on top of the Z-entropy LiDAR raster. This shows that the building footprints are well-aligned with the LiDAR rasters in order to ensure that the structures are not being considered in the LiDAR analysis.*\n",
    "\n",
    "### Import geometries and run HIZ script\n",
    "The function `simple_hiz` from `utils.HIZ` in this repo creates a single defensible space zone around each building footprint. Looking at the randomly selected buildings, 3m is a sufficient buffer. I will put this buffer in the parameters and say that the outer edge will be that buffer + 30m, so the outer edge will be 33m from the building footprint. \n",
    "\n",
    "30m is chosen because of the NFPA and Cohen definitions of defensible space that are supported by post fire assessments, such as the Lahaina post-fire investigation (Hedayati et al. 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53841908-5925-4775-99ea-c55c5a8df894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ouray County</td>\n",
       "      <td>POLYGON ((233065.876 4241049.368, 233062.927 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ouray County</td>\n",
       "      <td>POLYGON ((242799.828 4243540.283, 242798.994 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouray County</td>\n",
       "      <td>POLYGON ((251221.424 4245433.145, 251222.638 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ouray County</td>\n",
       "      <td>POLYGON ((244094.335 4241435.666, 244103.748 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ouray County</td>\n",
       "      <td>POLYGON ((254081.763 4242776.159, 254083.688 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         County                                           geometry\n",
       "0  Ouray County  POLYGON ((233065.876 4241049.368, 233062.927 4...\n",
       "1  Ouray County  POLYGON ((242799.828 4243540.283, 242798.994 4...\n",
       "2  Ouray County  POLYGON ((251221.424 4245433.145, 251222.638 4...\n",
       "3  Ouray County  POLYGON ((244094.335 4241435.666, 244103.748 4...\n",
       "4  Ouray County  POLYGON ((254081.763 4242776.159, 254083.688 4..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Import building footprints\n",
    "mbf = gpd.read_file(os.path.join(geoms, 'buildings_rs_WKID26913.shp'))\n",
    "mbf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997c01c3-9a1f-45ed-b610-7feba37d730e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "simple_hiz() missing 1 required positional argument: 'id_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHIZ\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple_hiz\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create defensible space zone around all building footprints, saving as its own gdf\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m hiz \u001b[38;5;241m=\u001b[39m \u001b[43msimple_hiz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmbf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m hiz\u001b[38;5;241m.\u001b[39mhead() \u001b[38;5;66;03m# Preview data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: simple_hiz() missing 1 required positional argument: 'id_column'"
     ]
    }
   ],
   "source": [
    "from utils.HIZ import simple_hiz\n",
    "\n",
    "# Create defensible space zone around all building footprints, saving as its own gdf\n",
    "hiz = simple_hiz(mbf)\n",
    "hiz.head() # Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7097a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "hiz.to_file(os.path.join(geoms, 'hiz_WKID26913.gpkg'), driver='GPKG', index=False)\n",
    "print('hiz geom saved to file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1c559",
   "metadata": {},
   "source": [
    "## Step 2: Structure Density\n",
    "---\n",
    "\n",
    "In order to obtain a proxy for structure density, we are going to count the number of adjacent structures in the vicinity of each home. We will also compute distance to nearest structure. We will append these counts to the HIZ geopackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff055bd-76ea-44c8-896b-98538cefe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of structures within defensible space\n",
    "from utils.HIZ import structures_in_hiz\n",
    "\n",
    "counts_df = structures_in_hiz(mbf, hiz)\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a10070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to csv in scratch folder\n",
    "counts_df.to_csv(os.path.join(scratch, 'structure_count.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute minimum structure separation distance per structure\n",
    "from utils.HIZ import min_ssd\n",
    "\n",
    "mbf = min_ssd(mbf)\n",
    "mbf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mbf to file in scratch folder\n",
    "mbf.to_file(os.path.join(scratch, 'mbf_wui_ssd.gpkg'), driver='GPKG', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff66adf",
   "metadata": {},
   "source": [
    "## Step 3: Get canopy cover info for each HIZ\n",
    "---\n",
    "\n",
    "### Read in LiDAR-derived rasters\n",
    "\n",
    "I created these rasters in R using the **lidR** package and methods documented in this repository under `r_workflows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60957165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-2m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "2-4m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "4-8m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "8+m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "chm:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "ladder:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "density:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N + NAVD88 height - US Geoid Model of 2018\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "zentropy:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from utils.raster import read_raster\n",
    "\n",
    "# Setup environment\n",
    "# ----\n",
    "# File naming conventions, starting with aoi (a county, community, etc.) and ending with spatial reference\n",
    "wkid = 'WKID26913'\n",
    "aoi = 'ouray'\n",
    "# ----\n",
    "\n",
    "# Initiate list to store objects\n",
    "lidar_rasters = []\n",
    "lidar_raster_profiles = []\n",
    "\n",
    "# Canopy cover 0-2m\n",
    "cc0_2, cc0_2_profile = read_raster(os.path.join(rasters, f'{aoi}_cc0_2m_{wkid}.tif'), layer=1, profile=True)\n",
    "lidar_rasters.append(cc0_2)\n",
    "lidar_raster_profiles.append(cc0_2_profile)\n",
    "\n",
    "# Canopy cover 2-4m\n",
    "cc2_4, cc2_4_profile = read_raster(os.path.join(rasters, f'{aoi}_cc2_4m_{wkid}.tif'), layer=1, profile=True)\n",
    "lidar_rasters.append(cc2_4)\n",
    "lidar_raster_profiles.append(cc2_4_profile)\n",
    "\n",
    "# Canopy cover 4-8m\n",
    "cc4_8, cc4_8_profile = read_raster(os.path.join(rasters, f'{aoi}_cc4_8m_{wkid}.tif'), layer=1, profile=True)\n",
    "lidar_rasters.append(cc4_8)\n",
    "lidar_raster_profiles.append(cc4_8_profile)\n",
    "\n",
    "# Canopy cover 8-40m\n",
    "cc8_40, cc8_40_profile = read_raster(os.path.join(rasters, f'{aoi}_cc8_40m_{wkid}.tif'), layer=1, profile=True)\n",
    "lidar_rasters.append(cc8_40)\n",
    "lidar_raster_profiles.append(cc8_40_profile)\n",
    "\n",
    "# Print metadata for quality inspection\n",
    "print('0-2m: ', cc0_2_profile)\n",
    "print('2-4m: ', cc2_4_profile)\n",
    "print('4-8m: ', cc4_8_profile)\n",
    "print('8+m: ', cc8_40_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64f03f-78b8-4e7a-b5d3-7ed96ebc5dcd",
   "metadata": {},
   "source": [
    "#### Get raster values per HIZ across the county\n",
    "\n",
    "Recall the geodataframes of home ignition zones that we created at the beginning of this workflow. It contains geometries for **XX** building footprins and their home ignition zones. We're going to look at the vegetation within each HIZ to get an idea of the vegetation profile around each home.\n",
    "\n",
    "For the purposes of reducing wildfire risk to individual homes, it's important that we analyze raster values within each buffer zone. GeoPandas comes in handy for this task.\n",
    "\n",
    "Our workflow for this calculation - written in plain English - is as follows:\n",
    "1. **Mask raster with geometries**: Recall that each home ignition zone gdf is a geoseries of Shapely geometries. We need to iterate through **each** of these **XX** objects and use them to mask our raster, object by object.\n",
    "2. **Calculate average raster values within each HIZ**: once the mask is applied to only include a single building buffer, we'll take the average of all the pixels in the area. \n",
    "3. **Store respective canopy cover value per HIZ as a new column in the HIZ dataframe:** The average values will be stored as columns and each value will correspond to the geometry in the same row.\n",
    "\n",
    "**TO DO:** decide the best way to organize these. I'm thinking column titles will be 'ccvalue_hiz' and will be appended to the individual HIZ shapefiles rather than the grandfather gdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in rasters\n",
    "# Mask rasters with hiz geom, one at a time, so the new scratch raster is the one for only the defensible space\n",
    "# Compute average (nanmean) of each scratch raster\n",
    "def calculate_avg(arr):\n",
    "    \n",
    "    # Count non-NaN values in the array\n",
    "    non_nan_count = np.sum(~np.isnan(arr))\n",
    "    # Compute the sum of non-NaN values\n",
    "    non_nan_sum = np.sum(arr[~np.isnan(arr)])\n",
    "    # Calculate the average (handle case when non_nan_count is 0 to avoid division by zero)\n",
    "    avg = non_nan_sum / non_nan_count if non_nan_count > 0 else np.nan\n",
    "\n",
    "    return avg\n",
    "# For each scratch raster, column should be named dynamically\n",
    "cc_rnames = ['cc0_2m', 'cc_2_4m', 'cc4_8m', 'cc8_40m']\n",
    "col_name = f'mean_{rname}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e7141",
   "metadata": {},
   "source": [
    "#### Append values to HIZ geopackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f847a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to append values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0a13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated GDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912afde3",
   "metadata": {},
   "source": [
    "## Discussion and Conclusions\n",
    "---\n",
    "\n",
    "- Broader picture of wildfire risk to communities and susceptibility and defensible space\n",
    "- Describe the project in its entirety, in brief\n",
    "- Here's how this notebook/workflow fits into the larger project\n",
    "- Here's what we did.\n",
    "- Here were the most difficult parts.\n",
    "- Here were the key parts to get right.\n",
    "- Time estimates for running the code.\n",
    "- Information on computational expense.\n",
    "- Room for improvement\n",
    "\n",
    "## NOTES: Possibble to use this code somewhere?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f6c1-ba55-49e5-835e-2fee694dd209",
   "metadata": {},
   "source": [
    "We're going to clip our rasters to the extent of the given geometry for each of these.\n",
    "\n",
    "Our simplified proxy for NFPA compliance is going to be the **average max height** of vegetation pixels within each zone.\n",
    "\n",
    "The process of clipping rasters to geometries requires a fe steps. First, we're going to mask the max height raster `arr_max` with each HIZ geometry - `z1`, `z2`, and `z3`. To do this, we have to rasterize each geometry. The following 3 code blocks accomplish that, and save the masked raster outputs to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3e50665-abb4-444a-b322-b23848de0270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "# Prepare geometries from GeoDataFrame for masking\n",
    "geoms = z1['geometry'].values # Zone 1\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z1_out_values, z1_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e6e13de-7457-4e23-ad1c-78b415b7c213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = z2['geometry'].values # Zone 2\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z2_out_values, z2_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05f46e07-e12e-40cd-a32d-b16f715aa52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = z3['geometry'].values # Zone 3\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z3_out_values, z3_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1bae4-8974-439a-87f6-da39ab7fc22b",
   "metadata": {},
   "source": [
    "Next, let's define a function for computing the average value of all the pixels.\n",
    "\n",
    "We know that our zones are all donut-shaped. If we divide the sum of the pixels by the size of the array, then pixels both inside and outside the donut will be included in the size, and that will throw off our calculation. The above 3 code blocks have set all the values outside and inside of the donuts to `NaN`, so now we make sure our `calculate_avg` function divided by the number of *non-NaN* pixels in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d55c73-0476-4254-b1d5-75bef0185fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4dcfc00-40ab-4ed4-b77d-c37b8daf5951",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "All that's left to do is calculate the average max height per zone. Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9437e5fe-49f8-4478-8307-e92b6ef6a5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3484971919686135"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z3_out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfb5257f-f10b-4329-beca-3332ddd82a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.054104777520576"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z2_out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bf84c2e-2682-4059-ae5d-f7767a5d7b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9157764165088382"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z1_out_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878ed2c-a044-4a3d-a19b-e14fec8ef2f5",
   "metadata": {},
   "source": [
    "### CONCLUSIONS\n",
    "\n",
    "There are some insights and some caveats to draw from this from a fire mitigation perspective.\n",
    "\n",
    "For a NFPA-compliant home ignition zone, we would expect to see taller average height values further away from the home. Instead, we are seeing the reverse.\n",
    "\n",
    "The Z1 values have possibly been skewed by the home itself. Since the Microsoft building footprint does not perfectly align with the location of the building in the point cloud, it is likely that the building itself has been included in the max height calculation. A way to handle this would be to classify the point cloud to include buildings, and derive the building footprint directly from the LiDAR data.\n",
    "\n",
    "The Z2 and Z3 values are probably more reliable. It tells us that there is tall vegetation surrounding the home. If the average height exceeds 3m in Z2, it is likely that this property could benefit from removing some trees and ensuring that the ground is clear of continuous flammable fuels. Since Z3 average max height is lower, it's likely that Z3 will not carry fire, although the average max height could be skewed by clumps of tall, continuous forest fuels interspersed with grassland, which is typical of this environment.\n",
    "\n",
    "My conclusion is that average max height does not produce an actionable insight for home risk reduction. There is potential to develop better ways of producing actionable insights for home wildfire risk mitigation, but none have been developed (based on a meticulous review of over 150 papers and models). While the results presented here may prove to be a useful predictor in a random forest or multi-layer perceptron model, I will continue to test the value of different LiDAR metrics using the framework developed through this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parcel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
