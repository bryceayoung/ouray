{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec583c1c-12ee-4fbb-951e-f0475ae51eac",
   "metadata": {},
   "source": [
    "# Ouray Defensible Space Analysis\n",
    "\n",
    "Author: **Bryce A Young** | \n",
    "Created: **2024-12-05** | \n",
    "Modified: **2024-12-17**\n",
    "\n",
    "In this notebook, we analyze fuel distributions within the defensible space of every building in Ouray County.\n",
    "\n",
    "This analysis workflow also documents cleaning and preparing raw data for the analysis.\n",
    "\n",
    "#### Data \n",
    "- (vector) Microsoft Building Footprints\n",
    "- (raster) SILVIS Global WUI raster\n",
    "- (raster) LiDAR-derived rasters\n",
    "\n",
    "#### Workflow \n",
    "- Create HIZ boundaries around each building footprint\n",
    "- Count number of homes within each HIZ\n",
    "- Obtain WUI class for each home\n",
    "- Get zonal summary values of each LiDAR-derived raster for each HIZ\n",
    "\n",
    "## Step 0: Setup Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f366a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "### Directory ###\n",
    "# Repository root\n",
    "root = 'E:/_PROJECTS/Ouray_ParcelRisk/ouray'\n",
    "\n",
    "### Data paths ###\n",
    "# Folder where all the data inputs and outputs will live\n",
    "data = 'E:/_PROJECTS/Ouray_ParcelRisk/data'\n",
    "# Folder for all SILVIS WUI files\n",
    "silvis_wui = os.path.join(data, 'silvis_wui')\n",
    "# Folder for microsoft building footprints\n",
    "mbf = os.path.join(data, 'building_footprints')\n",
    "# Folder containing LiDAR-derived rasters\n",
    "tiffs_from_las = os.path.join(data, 'tiffs_from_las')\n",
    "# Folder countaining the Ouray County boundary\n",
    "county_boundary = os.path.join(data, 'county_boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05a1593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\_PROJECTS\\\\Ouray_ParcelRisk\\\\ouray'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ef97a",
   "metadata": {},
   "source": [
    "## Step 1: Create HIZ Boundaries around each Structure\n",
    "---\n",
    "**Background: obtaining building footprints**  \n",
    "*I searched \"Ouray County Colorado Microsoft Building Footprints\" and found a Colorado state website that had parsed the Microsoft Building Footprints dataset into county datasets for all of Colorado. So I was able to directly download the Ouray footprints without having to do any data manipulation myself.*\n",
    "\n",
    "*After downloading the footprints, I added them to ArcGIS Pro and viewed them on top of my LiDAR data. Especially the `zentropy` layer shows buildings very well. I noticed that the footprint geometries are offset from the location of the buildings in the LiDAR rasters. So I used 'rubber sheeting' in ArcGIS Pro to put control points and target points that were intended to bring the building footprints closer to the buildings in the LiDAR rasters.*\n",
    "\n",
    "*The resulting shapefile was saved to `ouray_footprints_rs.shp` where 'rs' denotes 'rubber sheet' transformation of the original footprint layer. Original footprints are saved in the same directory as `Ouray_County_Buildings.shp`.*\n",
    "\n",
    "### Import geometries and run HIZ script\n",
    "\n",
    "**TO DO**: I have to re-do this rubber sheeting and make sure to save the result as its own shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53841908-5925-4775-99ea-c55c5a8df894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Import building footprints\n",
    "footprints = gpd.read_file(os.path.join(mbf, 'Ouray_County_Buildings.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c01c3-9a1f-45ed-b610-7feba37d730e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\_PROJECTS\\\\Ouray_ParcelRisk\\\\ouray\\\\workflows'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.HIZ import get_hiz\n",
    "\n",
    "# Create defensible space zones around all building footprints\n",
    "hiz = get_hiz(footprints)\n",
    "hiz.head() # Preview data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd62c1f",
   "metadata": {},
   "source": [
    "Now that we have created the HIZ boundaries around each building footprints, we need to save each one as its own geopackage for analysis. This could also be a shapefile, but a geopackage is more modern and generally preferred in 2025, so we'll stick with that.\n",
    "\n",
    "Because of the difference in alignment between buildings in the LiDAR data and the outlines provided by Microsoft Building Footprints, we're actually going to use this first buffer as an analog for the building itself. This will help avoid incuding the building pixels from the raster in the analysis.\n",
    "\n",
    "Below, we save each geometry column as its own geopackage, then we will restart the kernel to save memory and re-import the individual files.\n",
    "\n",
    "**TO DO**: If I am using buffer_z1 as the actual footprint, then I'm going to want to rename these to keep everything straight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each as its own geopackage\n",
    "\n",
    "# In order to save a shapefile of HIZs, each geometry column needs to be saved as a separate geopackage\n",
    "# Make sure each file contains one geometry column, and also 'FID' and \"County'.\n",
    "geom_cols = ['geometry', 'buffer_z1', 'buffer_z2', 'buffer_z3']\n",
    "non_geom_cols = ['FID', 'County']\n",
    "\n",
    "# Iterate and save each as a separate Shapefile\n",
    "for geom_col in geom_cols:\n",
    "    # Create a temporary GeoDataFrame with only the current geometry column and non-geometry columns\n",
    "    temp_gdf = hiz[non_geom_cols + [geom_col]].copy()\n",
    "    temp_gdf.set_geometry(geom_col, inplace=True)  # Loop through columns, setting each as the geometry column\n",
    "    \n",
    "    # Save to Shapefile\n",
    "    temp_gdf.to_file(f'E:\\_PROJECTS\\Ouray_ParcelRisk\\data\\hiz_geoms\\hiz_{geom_col}.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HIZ geoms into environment\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "z1 = gpd.read_file(\"hiz_buffer_z1.gpkg\")\n",
    "z2 = gpd.read_file(\"hiz_buffer_z2.gpkg\")\n",
    "z3 = gpd.read_file(\"hiz_buffer_z3.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1c559",
   "metadata": {},
   "source": [
    "## Step 2: Count the Number of Adjacent Structures\n",
    "---\n",
    "\n",
    "In order to obtain a proxy for structure density, we are going to count the number of adjacent structures in the vicinity of each home. We will append these counts to the geopackages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff055bd-76ea-44c8-896b-98538cefe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.HIZ import structures_per_zone\n",
    "\n",
    "buffer_cols = ['buffer_z1', 'buffer_z2', 'buffer_z3']  # Define your buffer zone columns\n",
    "counts_df = structures_per_zone(gdf, 'footprints', buffer_cols)\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc882c",
   "metadata": {},
   "source": [
    "**TO DO**: Does `counts_df` have geometries? Might need to add the extra step of appending that info to the footprint gdf. I think I ran this function in Gunnison so I should go look at those notebooks. \n",
    "\n",
    "## Step 3: Attributing WUI type to each home\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d640ad0",
   "metadata": {},
   "source": [
    "### Get WUI raster values per home\n",
    "\n",
    "I created a SILVIS WUI raster in `raster_prep.ipynb`, located in this repository. Now we will import the raster to the workspace as well as the building footprints. Then we're going to find the value of the raster at the centriod of each home. Those values will be appended to the HIZ geodataframe.\n",
    "\n",
    "Note that I will import the raster that is not clipped to the analysis area. This is because the AA does not encompass all of the building footprints. We won't be able to get LiDAR data for the homes outside the AA, but at least we can append the WUI info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "from utils.raster import read_raster\n",
    "import geopandas as gpd\n",
    "\n",
    "wui = read_raster('path.tif', layer=1)\n",
    "homes = gpd.read_file('path.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b283c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to do get the centriod of each geometry and get the raster value at the centriod\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "\n",
    "with rio.open('path.tif') as src:\n",
    "    # Extract geometry centroids from homes\n",
    "    centroids = homes.geometry.centroid\n",
    "    # Convert centroids to xy coords and store in list\n",
    "    points = [(geom.x, geom.y) for geom in centroids]\n",
    "    # Sample raster values at centroids\n",
    "    wui_values = [val[0] for val in src.sample(points)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append values to gdf\n",
    "homes['WUI'] = wui_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gdf to file\n",
    "homes.to_file('path.gpkg', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff66adf",
   "metadata": {},
   "source": [
    "## Step 4: Get canopy cover info for each HIZ\n",
    "---\n",
    "\n",
    "### Read in LiDAR-derived rasters\n",
    "\n",
    "I created these rasters in R using the **lidR** package and methods documented in this repository under `r_workflows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60957165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-2m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "2-4m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "4-8m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "8+m:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "chm:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "ladder:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "density:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N + NAVD88 height - US Geoid Model of 2018\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n",
      "zentropy:  {'driver': 'GTiff', 'dtype': 'float32', 'nodata': nan, 'width': 42881, 'height': 47000, 'count': 1, 'crs': CRS.from_wkt('LOCAL_CS[\"NAD83(2011) / UTM zone 13N\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(1.0, 0.0, 236950.0,\n",
      "       0.0, -1.0, 4247000.0), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from utils.raster import read_raster\n",
    "\n",
    "# Setup environment\n",
    "# ----\n",
    "# Path to all las-derived rasters\n",
    "tiffs_from_las = r\"E:/_PROJECTS/Ouray_ParcelRisk/data/tiffs_from_las\"\n",
    "# County, municipality etc. where the rasters cover\n",
    "aoi = 'ouray'\n",
    "# ----\n",
    "\n",
    "# Initiate list to store objects\n",
    "lidar_rasters = []\n",
    "lidar_raster_profiles = []\n",
    "\n",
    "# Canopy cover 0-2m\n",
    "cc0_2, cc0_2_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc0_2m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc0_2)\n",
    "lidar_raster_profiles.append(cc0_2_profile)\n",
    "\n",
    "# Canopy cover 2-4m\n",
    "cc2_4, cc2_4_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc2_4m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc2_4)\n",
    "lidar_raster_profiles.append(cc2_4_profile)\n",
    "\n",
    "# Canopy cover 4-8m\n",
    "cc4_8, cc4_8_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc4_8m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc4_8)\n",
    "lidar_raster_profiles.append(cc4_8_profile)\n",
    "\n",
    "# Canopy cover 8-40m\n",
    "cc8_40, cc8_40_profile = read_raster(os.path.join(tiffs_from_las, '{0}_cc8_40m.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(cc8_40)\n",
    "lidar_raster_profiles.append(cc8_40_profile)\n",
    "\n",
    "# Canopy height model\n",
    "chm, chm_profile = read_raster(os.path.join(tiffs_from_las, '{0}_chm.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(chm)\n",
    "lidar_raster_profiles.append(chm_profile)\n",
    "\n",
    "# Ladder fuels\n",
    "ladder, ladder_profile = read_raster(os.path.join(tiffs_from_las, '{0}_ladderfuels.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(ladder)\n",
    "lidar_raster_profiles.append(ladder_profile)\n",
    "\n",
    "# Point density\n",
    "density, density_profile = read_raster(os.path.join(tiffs_from_las, '{0}_pointDensity.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(density)\n",
    "lidar_raster_profiles.append(density_profile)\n",
    "\n",
    "# Z-Entropy\n",
    "zentropy, zentropy_profile = read_raster(os.path.join(tiffs_from_las, '{0}_zentropy.tif'.format(aoi)), layer=1, profile=True)\n",
    "lidar_rasters.append(zentropy)\n",
    "lidar_raster_profiles.append(zentropy_profile)\n",
    "\n",
    "# Print metadata for quality inspection\n",
    "print('0-2m: ', cc0_2_profile)\n",
    "print('2-4m: ', cc2_4_profile)\n",
    "print('4-8m: ', cc4_8_profile)\n",
    "print('8+m: ', cc8_40_profile)\n",
    "print('chm: ', chm_profile)\n",
    "print('ladder: ', ladder_profile)\n",
    "print('density: ', density_profile)\n",
    "print('zentropy: ', zentropy_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64f03f-78b8-4e7a-b5d3-7ed96ebc5dcd",
   "metadata": {},
   "source": [
    "#### Get raster values per HIZ across the county\n",
    "\n",
    "Recall the geodataframes of home ignition zones that we created at the beginning of this workflow. It contains geometries for **XX** building footprins and their home ignition zones. We're going to look at the vegetation within each HIZ to get an idea of the vegetation profile around each home.\n",
    "\n",
    "For the purposes of reducing wildfire risk to individual homes, it's important that we analyze raster values within each buffer zone. GeoPandas comes in handy for this task.\n",
    "\n",
    "Our workflow for this calculation - written in plain English - is as follows:\n",
    "1. **Mask raster with geometries**: Recall that each home ignition zone gdf is a geoseries of Shapely geometries. We need to iterate through **each** of these **XX** objects and use them to mask our raster, object by object.\n",
    "2. **Calculate average raster values within each HIZ**: once the mask is applied to only include a single building buffer, we'll take the average of all the pixels in the area. \n",
    "3. **Store respective canopy cover value per HIZ as a new column in each HIZ dataframe:** The average values will be stored as columns and each value will correspond to the geometry in the same row.\n",
    "\n",
    "**TO DO:** decide the best way to organize these. I'm thinking column titles will be 'ccvalue_hiz' and will be appended to the individual HIZ shapefiles rather than the grandfather gdf.\n",
    "\n",
    "THIS IS IN `ndvi_hiz.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average raster value per HIZ\n",
    "def calculate_avg(arr):\n",
    "    \n",
    "    # Count non-NaN values in the array\n",
    "    non_nan_count = np.sum(~np.isnan(arr))\n",
    "    # Compute the sum of non-NaN values\n",
    "    non_nan_sum = np.sum(arr[~np.isnan(arr)])\n",
    "    # Calculate the average (handle case when non_nan_count is 0 to avoid division by zero)\n",
    "    avg = non_nan_sum / non_nan_count if non_nan_count > 0 else np.nan\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e7141",
   "metadata": {},
   "source": [
    "#### Append values to HIZ geopackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f847a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to append values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0a13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated GDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912afde3",
   "metadata": {},
   "source": [
    "## Discussion and Conclusions\n",
    "---\n",
    "\n",
    "- Broader picture of wildfire risk to communities and susceptibility and defensible space\n",
    "- Describe the project in its entirety, in brief\n",
    "- Here's how this notebook/workflow fits into the larger project\n",
    "- Here's what we did.\n",
    "- Here were the most difficult parts.\n",
    "- Here were the key parts to get right.\n",
    "- Time estimates for running the code.\n",
    "- Information on computational expense.\n",
    "- Room for improvement\n",
    "\n",
    "## NOTES: Possibble to use this code somewhere?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f6c1-ba55-49e5-835e-2fee694dd209",
   "metadata": {},
   "source": [
    "We're going to clip our rasters to the extent of the given geometry for each of these.\n",
    "\n",
    "Our simplified proxy for NFPA compliance is going to be the **average max height** of vegetation pixels within each zone.\n",
    "\n",
    "The process of clipping rasters to geometries requires a fe steps. First, we're going to mask the max height raster `arr_max` with each HIZ geometry - `z1`, `z2`, and `z3`. To do this, we have to rasterize each geometry. The following 3 code blocks accomplish that, and save the masked raster outputs to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3e50665-abb4-444a-b322-b23848de0270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "# Prepare geometries from GeoDataFrame for masking\n",
    "geoms = z1['geometry'].values # Zone 1\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z1_out_values, z1_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e6e13de-7457-4e23-ad1c-78b415b7c213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = z2['geometry'].values # Zone 2\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z2_out_values, z2_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05f46e07-e12e-40cd-a32d-b16f715aa52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = z3['geometry'].values # Zone 3\n",
    "\n",
    "# Create a new in-memory dataset\n",
    "with MemoryFile() as memfile:\n",
    "    # Define metadata based on the properties of arr_max\n",
    "    meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'count': 1,\n",
    "        'width': arr_max.shape[1],\n",
    "        'height': arr_max.shape[0],\n",
    "        'crs': hmax.crs,  # Update with the correct CRS as necessary\n",
    "        'transform': hmax.transform,  # Update with the correct transform if available\n",
    "        'nodata': np.nan\n",
    "    }\n",
    "\n",
    "    with memfile.open(**meta) as hmax_dataset:\n",
    "        # Write arr_max to the in-memory dataset\n",
    "        hmax_dataset.write(arr_max, 1)\n",
    "\n",
    "        # Apply the mask using the geometries\n",
    "        z3_out_values, z3_out_transform = mask(hmax_dataset, geoms, crop=True, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1bae4-8974-439a-87f6-da39ab7fc22b",
   "metadata": {},
   "source": [
    "Next, let's define a function for computing the average value of all the pixels.\n",
    "\n",
    "We know that our zones are all donut-shaped. If we divide the sum of the pixels by the size of the array, then pixels both inside and outside the donut will be included in the size, and that will throw off our calculation. The above 3 code blocks have set all the values outside and inside of the donuts to `NaN`, so now we make sure our `calculate_avg` function divided by the number of *non-NaN* pixels in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d55c73-0476-4254-b1d5-75bef0185fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4dcfc00-40ab-4ed4-b77d-c37b8daf5951",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "All that's left to do is calculate the average max height per zone. Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9437e5fe-49f8-4478-8307-e92b6ef6a5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3484971919686135"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z3_out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfb5257f-f10b-4329-beca-3332ddd82a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.054104777520576"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z2_out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bf84c2e-2682-4059-ae5d-f7767a5d7b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9157764165088382"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_avg(z1_out_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878ed2c-a044-4a3d-a19b-e14fec8ef2f5",
   "metadata": {},
   "source": [
    "### CONCLUSIONS\n",
    "\n",
    "There are some insights and some caveats to draw from this from a fire mitigation perspective.\n",
    "\n",
    "For a NFPA-compliant home ignition zone, we would expect to see taller average height values further away from the home. Instead, we are seeing the reverse.\n",
    "\n",
    "The Z1 values have possibly been skewed by the home itself. Since the Microsoft building footprint does not perfectly align with the location of the building in the point cloud, it is likely that the building itself has been included in the max height calculation. A way to handle this would be to classify the point cloud to include buildings, and derive the building footprint directly from the LiDAR data.\n",
    "\n",
    "The Z2 and Z3 values are probably more reliable. It tells us that there is tall vegetation surrounding the home. If the average height exceeds 3m in Z2, it is likely that this property could benefit from removing some trees and ensuring that the ground is clear of continuous flammable fuels. Since Z3 average max height is lower, it's likely that Z3 will not carry fire, although the average max height could be skewed by clumps of tall, continuous forest fuels interspersed with grassland, which is typical of this environment.\n",
    "\n",
    "My conclusion is that average max height does not produce an actionable insight for home risk reduction. There is potential to develop better ways of producing actionable insights for home wildfire risk mitigation, but none have been developed (based on a meticulous review of over 150 papers and models). While the results presented here may prove to be a useful predictor in a random forest or multi-layer perceptron model, I will continue to test the value of different LiDAR metrics using the framework developed through this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parcel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
